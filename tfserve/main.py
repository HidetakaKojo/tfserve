"""Provides tfserve main (script) support.

"""
import argparse
import json
import socket
import sys

from werkzeug.exceptions import BadRequest

from tfserve.tfserve import TFServeApp
from tfserve.tfserve import BadInput
from tfserve import helper

DEFAULT_HOST = '0.0.0.0'
DEFAULT_PORT = 5000

HELP = """
Serve a TensorFlow model.

Inputs are submitted to the root path '/' using HTTP POST and contain
a JSON encoded map of input tensors to input values. Input values must
comply with the expected tensors shape. If batch mode is enabled,
multiple input values must be submitted in an array.

A response body is a JSON encoded map of output tensors to output
values generated by the model for the given the inputs. If batch mode
is enabled, output values are returned in an array.

To run in batch mode, use --batch.

For help with model input and output tensors, use --help-model.
"""

class EncodeDecodeHandler():
    """
    Handler for encode and decode.
    """

    def __init__(self, input_tensors, output_tensors, batch_mode):
        self.input_tensors = input_tensors
        self.output_tensors = output_tensors
        self.batch_mode = batch_mode

    def encode(self, request_bytes):
        """Encode request bytes as model inputs.

        `request_bytes` is a byte array that can be decoded as
        JSON. The decoded value must be a Python dict that contains
        values for each input tensor.

        """
        inputs = self._decode_request(request_bytes)
        self._validate_inputs(inputs)
        return inputs

    @staticmethod
    def _decode_request(request_bytes):
        if not request_bytes:
            raise BadInput("empty request")
        try:
            request_str = request_bytes.decode('utf-8')
        except UnicodeDecodeError as e:
            raise BadInput(str(e))
        else:
            try:
                return json.loads(request_str)
            except json.decoder.JSONDecodeError as e:
                raise BadInput(str(e))

    def _validate_inputs(self, inputs):
        if not isinstance(inputs, dict):
            raise BadInput("inputs must be a JSON object")
        missing_inputs = [
            name for name in self.input_tensors
            if name not in inputs]
        if missing_inputs:
            raise BadInput(
                "missing inputs: %s"
                % ','.join(missing_inputs))

    @staticmethod
    def decode(outputs):
        """Decodes model outputs to a JSON serializable Python dict.

        """
        return {
            name: outputs[name].tolist()
            for name in outputs
        }

def main():
    """TFServe main function.

    """
    maybe_help_args = _init_args(require_tensors=False)
    if maybe_help_args.help_model:
        _show_model_help_and_exit(maybe_help_args)
    serve_args = _init_args(require_tensors=True)
    _serve_model(serve_args)

def _init_args(require_tensors):
    p = argparse.ArgumentParser(
        description=HELP,
        formatter_class=argparse.RawTextHelpFormatter)
    p.add_argument(
        '-m', '--model', metavar='PATH', required=True,
        help="path to pb file or directory containing checkpoint")
    p.add_argument(
        '-i', '--inputs', required=require_tensors,
        help="a comma separated list of input tensors")
    p.add_argument(
        '-o', '--outputs', required=require_tensors,
        help="a comma separated list of output tensors")
    p.add_argument(
        '-b', '--batch', action='store_true',
        help=(
            "process multiple inputs (default is to process\n"
            "one input per request)"))
    p.add_argument(
        '-H', '--host', default=DEFAULT_HOST,
        help="host interface to bind to (%s)" % DEFAULT_HOST)
    p.add_argument(
        '-p', '--port', default=DEFAULT_PORT,
        help="port to listen on (%i)" % DEFAULT_PORT)
    p.add_argument(
        '--help-model', action='store_true',
        help=(
            "show model help for PATH and exit; use to list\n"
            "possible input and output tensors"))
    return p.parse_args()

def _show_model_help_and_exit(args):
    helper.estimate_io_tensors(args.model)
    sys.exit(0)

def _serve_model(args):
    inputs = _split_tensors(args.inputs)
    outputs = _split_tensors(args.outputs)
    handler = EncodeDecodeHandler(inputs, outputs, args.batch)
    app = TFServeApp(
        args.model,
        inputs,
        outputs,
        handler.encode,
        handler.decode,
        args.batch)
    sys.stdout.write("Running at {}\n".format(_serve_url(args)))
    app.run(args.host, args.port, _middleware)
    sys.stdout.write('\n')

def _split_tensors(tensors):
    return [s.strip() for s in tensors.split(',')]

def _serve_url(args):
    host = args.host
    if not host or host == "0.0.0.0":
        host = socket.gethostname()
        try:
            # Verify that configured hostname is valid
            socket.gethostbyname(host)
        except socket.gaierror:
            host = 'localhost'
    return 'http://%s:%i' % (host, args.port)

def _middleware(handler, req):
    try:
        return handler(req)
    except ValueError as e:
        raise BadRequest(str(e))

if __name__ == '__main__':
    main()
